\documentclass[12pt, fleqn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amssymb, amsmath, mathrsfs, amsthm}
\usepackage[russian]{babel}
\usepackage{graphicx}
\usepackage[footnotesize]{caption2}
\usepackage{indentfirst}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow}
%\usepackage[ruled,section]{algorithm}
%\usepackage[noend]{algorithmic}
%\usepackage[all]{xy}

% Параметры страницы
\textheight=24cm
\textwidth=16cm
\oddsidemargin=5mm
\evensidemargin=-5mm
\marginparwidth=36pt
\topmargin=-1cm
\footnotesep=3ex
%\flushbottom
\raggedbottom
\tolerance 3000
% подавить эффект "висячих стpок"
\clubpenalty=10000
\widowpenalty=10000
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\baselinestretch}{1.5} %для печати с большим интервалом

% Дополнительные команды для личных обозначений
\newcommand{\expectation}{\mathop{\mathbb{E}}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\loss}{\mathop{\mathcal{L}}}
\newcommand{\mse}{\mathop{MSE}}
\newcommand{\scalarproduct}[1]{\langle #1 \rangle}

\newcommand{\predictionfunction}{\hat{f}}
\newcommand{\ensemblefunction}{a}
\newcommand{\optimizationmethodfunction}{\tilde{f}}
\newcommand{\distinguishparameter}{\alpha}
\newcommand{\objects}{X}
\newcommand{\results}{Y}
\newcommand{\predictedobjects}{\widehat{\objects}}

\newcommand{\numberobjects}{N}
\newcommand{\numberpredictionfunctions}{M}

\newcommand{\for}[3]{\sum\limits_{#1 = #2}^{#3}}  % Usage: \for{index}{begin}{end}
\newcommand{\forn}[2]{\for{#1}{1}{#2}}  % Usage: \forn{index}{end}

\newcommand{\many}[3]{#1 1 #2, #1 2 #2, \dots, #1 #3 #2}  % Usage: \many{prefix}{suffix}{end}

\newcommand{\reference}[1]{(\hyperref[#1]{\ref{#1}})}

\newcommand{\ensemblefunctionfull}{\ensemblefunction(\many{\predictionfunction_}{(x)}{\numberpredictionfunctions})}

\begin{document}

\begin{titlepage}
\begin{center}
    Московский государственный университет имени М. В. Ломоносова

    \bigskip
    \includegraphics[width=50mm]{msu.eps}

    \bigskip
    Факультет Вычислительной Математики и Кибернетики\\
    Кафедра Математических Методов Прогнозирования\\[10mm]

    \textsf{\large\bfseries
        КУРСОВАЯ РАБОТА СТУДЕНТА 317 ГРУППЫ\\[10mm]
        <<Методы повышения эффективности обучения, основанные на~ансамблях промежуточных~решений>>
    }\\[10mm]

    \begin{flushright}
        \parbox{0.5\textwidth}{
            Выполнил:\\
            студент 3 курса 317 группы\\
            \emph{Королев Николай Сергеевич}\\[5mm]
            Научный руководитель:\\
            д.ф-м.н., в. науч. сотр. ВЦ~РАН\\
            \emph{Сенько Олег Валентинович}
        }
    \end{flushright}

    \vspace{\fill}
    Москва, 2019
\end{center}
\end{titlepage}

\newpage
\renewcommand{\contentsname}{Содержание}
\tableofcontents

\newpage
\begin{abstract}
    Данный документ является образцом оформления дипломной работы для студентов кафедры 
    Математических методов прогнозирования ВМК~МГУ. 
    Приведённые ниже рекомендации взяты из~статьи
    <<Написание отчётов и статей (рекомендации)>>
    на~вики"~ресурсе \texttt{www.MachineLearning.ru}.
    Студенты, готовящие дипломную работу к~защите, 
    могут найти много полезной информации также в~статьях 
    <<Научно-исследовательская работа (рекомендации)>>,
    <<Подготовка презентаций (рекомендации)>>,
    <<Защита выпускной квалификационной работы (рекомендации)>>
    на~том~же ресурсе. 

    Аннотация обычно содержит 
    краткое описание постановки задачи и~полученных результатов,
    одним абзацем на 10--15 строк.
    Цель аннотации "--- обозначить в~общих чертах, о~чём работа,
    чтобы человек, совершенно не~знакомый с~данной работой,
    понял, интересна~ли ему эта тема, и~стоит~ли читать дальше.
    Аннотация собирается в~последнюю очередь
    путем легкой модификации наиболее важных и~удачных фраз из введения и~заключения.
\end{abstract}

\newpage
\section{Введение}

Во введении рассказывается, где возникает данная задача, и~почему её решение так важно.
Вводится на неформальном уровне минимум терминов, необходимый для понимания постановки задачи.
Приводится краткий анализ источников информации (литературный обзор):
как эту задачу решали до сих пор, в~чем недостаток этих решений, и~что нового предлагает автор.
Формулируются цели исследования. 
В~конце введения даётся краткое содержание работы по разделам; 
при этом отмечается, какие подходы, методы, алгоритмы предлагаются автором впервые. 
При~упоминании ключевых разделов кратко формулируются основные результаты и~наиболее важные выводы.

Цель введения: дать достаточно полное представление о~выполненном исследовании 
и~полученных результатах, понятное широкому кругу специалистов. 
Большинство читателей прочтут именно введение и, быть может, заключение. 
Во~введении автор решает сложную оптимизационную проблему: 
как сообщить только самое важное, потратив минимум времени читателя,
да~так, чтобы максимум читателей поняли, о~чём вообще идёт речь.

Введение лучше писать напоследок, так как в~ходе работы обычно происходит переосмысление постановки задачи.
Если же введение писать, когда работа еще не~готова, задача усложняется вдвойне.
В~конце обычно приходит понимание, что всё получилось совсем не~так, как планировалось в~начале,
и~исходный вариант введения всё равно придётся переписывать. 
Кстати, к~таким «потерям» надо относиться спокойно~--- в~хорошей работе почти каждый абзац многократно переделывается до~неузнаваемости.
 
Введение имеет много общего с~текстом доклада на~защите, поэтому имеет смысл готовить их одновременно.

\subsection{Постановка задачи}

$\many{x_}{}{\numberobjects}$~--~точки в некотором векторном пространстве; $\many{y_}{}{\numberobjects}$~--~ значения, соответствующие этим точкам. При этом $y_i = y(x_i) = f(x_i) + \varepsilon$, где $\varepsilon$~--~случайная величина с нулевым математическим ожиданием и дисперсией $\sigma^2$. Пусть также задана функция $\predictionfunction_1(x)$, которая приближает функцию $f(x)$. Стоит задача нахождения $\numberpredictionfunctions - 1$ функции $\predictionfunction_2(x), \predictionfunction_3(x), \dots, \predictionfunction_\numberpredictionfunctions(x)$, а также функции-ансамбля $\predictionfunction(x) = \ensemblefunctionfull$, приближающей функцию $f(x)$ лучше чем любая из функций $\predictionfunction_i(x), \; i = \many{}{}{\numberpredictionfunctions}$.

\subsection{Теоретическая часть}
Из~\cite{BiasVarianceDecompositionZeroOneSquaredLoss} известно разложение математического ожидания квадратичной ошибки на смещение и дисперсию:
\begin{equation*}
\expectation \left[ \left( \predictionfunction(x) - y \right)^2 \right] =
\left(
	\expectation \predictionfunction(x) - f(x)
\right)^2  +
\left(
	\expectation \predictionfunction^2(x) - \left(\expectation \predictionfunction(x)\right)^2
\right) + \sigma^2
\end{equation*}

Для уменьшения как смещения, так и дисперсии $\predictionfunction(x)$ используются ансамбли:
\begin{equation*}
\predictionfunction(x) = \ensemblefunctionfull
\end{equation*}

В~\cite{OptimalConvexCorrectingProcedures} было доказано, что в случае, когда функция $\ensemblefunctionfull$ является выпуклой комбинацией своих аргументов:
\begin{equation*}
\ensemblefunctionfull = 
\forn{i}{\numberpredictionfunctions} c_i \predictionfunction_i(x),
\end{equation*}
\begin{equation*}
\forn{i}{\numberpredictionfunctions} c_i = 1; \quad c_i \geq 0, \quad i = \many{}{}{\numberpredictionfunctions};
\end{equation*}
выполнено
\begin{equation}\label{OptimizedDifferenceDecomposition}
\expectation \left[\left( \predictionfunction(x) - y \right)^2 \right] =
\forn{i}{\numberpredictionfunctions} c_i
\expectation \left[ \left(
	\predictionfunction_i(x) - y
\right)^2 \right] 
- \dfrac{1}{2}
\forn{i}{\numberpredictionfunctions} \forn{j}{\numberpredictionfunctions} c_i c_j
\expectation \left[ \left(
	\predictionfunction_i(x) - \predictionfunction_j(x)
\right)^2 \right]
\end{equation}

Соответственно, для уменьшения среднеквадратичной ошибки необходимо уменьшать среднеквадратичную ошибку каждого предиктора $\predictionfunction_i(x)$, а также увеличивать расхождение между прогнозами различных предикторов.

\subsection{Существующие методы}

В большинстве случаев функция-ансамбль $\predictionfunction(x) = \ensemblefunctionfull$ является линейной комбинацией функций предикторов $\many{\predictionfunction_}{(x)}{\numberpredictionfunctions}$. Коэффициенты ищутся при помощи методов регрессионного анализа:
\begin{itemize}
\item гребневая регрессия~\cite{Ridge}
\item метод Лассо~\cite{Lasso}
\item эластичная сеть~\cite{ElasticNet}
\item регрессионная модель, отбирающая признаки, наиболее коррелирующие с откликом~\cite{ConvexCombinationsBestCorrelatedWithResponse}
\end{itemize}

Функции $\many{\predictionfunction_}{(x)}{\numberpredictionfunctions}$ в таких случаях обычно независимы друг от друга и получены методом оптимизации некоторого функционала.

\section{Ансамбль промежуточных решений}

Пусть:\\
$\objects$ - матрица из $\numberobjects$ строк, $i$-ая строка равна $x_i$;\\
$\results$ - вектор из $\numberobjects$ элементов, $i$-ый элемент равен $y_i$.

Пусть $\predictionfunction_1(x)$ представима в виде функции с параметрами $\optimizationmethodfunction(x, \theta)$ и была получена некоторым методом оптимизации функционала среднеквадратичной ошибки
$\mse(\predictionfunction_1(x), \objects, \results) = \dfrac{1}{\numberobjects}\forn{i}{\numberobjects} (\predictionfunction_1(x_i) - y_i)^2$:
$$\predictionfunction_1(x) = \optimizationmethodfunction(x, \theta_1)$$

В качестве функции $\optimizationmethodfunction(x, \theta)$ может выступать нейронная сеть, тогда параметрами $\theta$ будут являться веса нейронной сети.

\subsection{Поиск промежуточных решений}

Будем искать функции $\predictionfunction_2(x), \predictionfunction_3(x), \dots, \predictionfunction_\numberpredictionfunctions(x)$ в виде $\optimizationmethodfunction(x, \theta)$, минимизируя следующий функционал:
$$
\loss(\predictionfunction_k(x), \objects, \results) = \mse(\optimizationmethodfunction(x, \theta_k), \objects, \results) -
\dfrac{\distinguishparameter}{k - 1}
\forn{i}{k - 1}\norm{\theta_k - \theta_i}^2, \quad k = 2, 3, \dots, \numberpredictionfunctions,
$$
где $\distinguishparameter \geq 0$ является гиперпараметром.

Данный функционал поощряет функции $\predictionfunction_i(x)$ иметь различные параметры $\theta_i$. Таким образом, мы пытаемся добиться максимального расхождения значений функций. Впоследствии это уменьшит среднеквадратичную ошибку функции-ансамбля $\predictionfunction(x)$, что следует из уравнения \reference{OptimizedDifferenceDecomposition}.

Стоит отметить, что при больших $k$ вычисление данного функционала может быть вычислительно затратным, так как $\forn{i}{k - 1}\norm{\theta_k - \theta_i}^2$ в общем случае требует $O(k)$ времени. Если все $\theta_i$ лежат в некотором евклидовом пространстве, данное выражение переписывается в виде:
$$
\forn{i}{k - 1}\norm{\theta_k - \theta_i}^2 = (k - 1)\norm{\theta_k}^2 - 2\scalarproduct{\theta_k, \forn{i}{k - 1} \theta_i} + \forn{i}{k - 1}\norm{\theta_i}^2.
$$

Значения выражений $\forn{i}{k - 1} \theta_i$ и $\forn{i}{k - 1}\norm{\theta_i}^2$ можно поддерживать в течении всего процесса поиска функции $\predictionfunction_k(x)$ и пересчитывать за $O(1)$ времени при переходе к поиску~$\predictionfunction_{k+1}(x)$.

\subsection{Поиск функции-ансамбля}

Составим матрицу $\predictedobjects$ размера $\numberobjects \times \numberpredictionfunctions$, где $\predictedobjects_{i, j} = \predictionfunction_j(x_i)$.
Для поиска функции-ансамбля $\predictionfunction(x) = \ensemblefunctionfull$ необходимо найти функцию $\ensemblefunction(\many{\hat{x}_}{}{\numberpredictionfunctions}) = \ensemblefunction(\hat{x})$, которую будем искать
из минимизации функционала $\mse(\ensemblefunction(\hat{x}), \predictedobjects, \results)$.

\section{Вычислительные эксперименты}

Для проверки качества работы представленного метода использовались 3 выборки данных с историческим артериальным давлением. Данные заранее разбиты на обучающую и тестовую выборки. Размеры каждой из выборок представлены в таблице \ref{tabular:datasize}. Все параметры являются численными.

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\multirow{2}{*}{Номер выборки} & \multicolumn{2}{c|}{Количество объектов} & \multirow{2}{*}{Количество параметров} \\
			\cline{2-3}
			& Обучающая выборка & Тестовая выборка & \\
			\hline
			1 & 718 & 458 & 114 \\
			\hline
			2 & 373 & 221 & 114 \\
			\hline
			3 & 832 & 495 & 114 \\
			\hline
		\end{tabular}
	\end{center}
	
	\caption{Размеры выборок данных}
	\label{tabular:datasize}
\end{table}

В качестве функции $\optimizationmethodfunction(x, \theta)$ используется двухслойная нейронная сеть с сигмоидальной функцией активации, которая оптимизируется методом Adam. В качестве функции-ансамбля $\ensemblefunction(\hat{x})$ используется точно такая же нейронная сеть, с другим количеством входных параметров и другой размерностью скрытого слоя.

В ходе эксперимента изначально обучается одна нейронная сеть $\predictionfunction_1(x) = \optimizationmethodfunction(x, \theta_1)$ и оценивается качество её работы. После этого обучается $\numberpredictionfunctions$ новых нейронных сетей $\predictionfunction_i(x) = \optimizationmethodfunction_i(x, \theta_i)$, а также ищется оптимальная функция-ансамбль $\ensemblefunction(\hat{x})$ в виде двухслойной нейронной сети. Затем оценивается качество работы ансамбля промежуточных решений $\predictionfunction(x) = \ensemblefunctionfull$.

\subsection{Результаты эксперимента}

Наилучших результатов удалось достичь для $\numberpredictionfunctions = 4$. Также при построении ансамбля промежуточных решений был замечен эффект улучшения качества предсказания отдельных нейронных сетей, обученных после первой нейронной сети $\predictionfunction_1(x)$.
Итоговые результаты эксперимента представлены в таблице \ref{tabular:results}.

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			Номер выборки & Одна нейронная сеть & Ансамбль промежуточных решений \\
			\hline
			1 & 0.78 & 0.86 \\
			\hline
			2 & 0.79 & 0.83 \\
			\hline
			3 & 0.78 & 0.81 \\
			\hline
		\end{tabular}
	\end{center}
	
	\caption{Результаты экспериментов (ROC AUC на тестовой выборке)}
	\label{tabular:results}
\end{table}

\subsection{Обсуждение и~выводы}
Приводятся выводы: 
в~какой степени результаты экспериментов согласуются с~теорией? 
Достигнут ли желаемый результат? 
Обнаружены ли какие-либо факты, не~нашедшие объяснения, и~которые нельзя списать на «грязный» эксперимент?

Обсуждаются основные отличия предложенных методов от известных ранее. 
В~чем их преимущества? 
Каковы границы их применимости? 
Какие проблемы удалось решить, а~какие остались открытыми? 
Какие возникли новые постановки задач?

\section{Заключение}

В~квалификационных работах последний раздел нужен для того, чтобы 
конспективно перечислить основные результаты, полученные лично автором. 

Результатами, в~частности, являются:
\begin{itemize}
\item 
    Предложен новый подход к\dots
\item 
    Разработан новый метод\dots, позволяющий\dots
\item 
    Доказан ряд теорем, подтверждающих (опровергающих), что\dots
\item 
    Проведены вычислительные эксперименты\dots,
    которые подтвердили / опровергли / привели к~новым постановкам задач.
\end{itemize}
    
Цель данного раздела: доказать квалификацию автора. 
Даже беглого взгляда на заключение должно быть достаточно, чтобы стало ясно: 
автору удалось решить актуальную, трудную, ранее не~решённую задачу, 
предложенные автором решения обоснованы и~проверены.

Иногда в~Заключении приводится список направлений дальнейших исследований.

\newpage

\renewcommand{\bibname}{Список литературы}
\addcontentsline{toc}{section}{\bibname}

\nocite{OptimalConvexCorrectingProcedures, ConvexCombinationsBestCorrelatedWithResponse}

\def\BibUrl#1.{}\def\BibAnnote#1.{}
%\def\BibUrl#1{\\{\footnotesize\tt\def~{\char126} http://#1}}
\bibliographystyle{gost71s}
\bibliography{bibliography}

\end{document}
